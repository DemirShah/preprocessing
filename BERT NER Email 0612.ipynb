{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751da8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "\n",
    "    def __init__(self, texts, tags, tokenizer, tag2id, max_len=512, label_all_tokens=False):\n",
    "        self.texts = texts  # Список токенизированных текстов (списки слов)\n",
    "        self.tags = tags  # Список последовательностей тегов, соответствующих текстам\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tag2id = tag2id  # Словарь, отображающий строки тегов в их ID\n",
    "        self.max_len = max_len\n",
    "        self.label_all_tokens = label_all_tokens  # Отмечать ли все подсловные токены метками\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words = self.texts[idx]\n",
    "        labels = self.tags[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            words,\n",
    "            is_split_into_words=True,\n",
    "            return_offsets_mapping=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attenti  on_mask'].squeeze()\n",
    "        offset_mapping = encoding['offset_mapping'].squeeze()\n",
    "\n",
    "        # Получаем идентификаторы слов для каждого токена во входной последовательности\n",
    "        word_ids = encoding.word_ids(batch_index=0)\n",
    "\n",
    "        # Выравниваем метки с токенами\n",
    "        aligned_labels = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                # Специальные токены (например, [CLS], [SEP]) имеют word_id равный None\n",
    "                aligned_labels.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # Метка для первого токена слова\n",
    "                aligned_labels.append(self.tag2id[labels[word_idx]])\n",
    "            else:\n",
    "                # Для подсловных токенов (частей слова после разбиения)\n",
    "                if self.label_all_tokens:\n",
    "                    aligned_labels.append(self.tag2id[labels[word_idx]])\n",
    "                else:\n",
    "                    # Игнорируем метки для остальных частей слова\n",
    "                    aligned_labels.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels = torch.tensor(aligned_labels, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09022566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertForTokenClassification, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "class BertNERTrainer:\n",
    "\n",
    "    def __init__(self, model_path, tokenizer_path, tag2id, epochs=1, model_save_path='bert_ner.pt'):\n",
    "        self.model = BertForTokenClassification.from_pretrained(\n",
    "            model_path,\n",
    "            num_labels=len(tag2id)\n",
    "        )\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model_save_path = model_save_path\n",
    "        self.max_len = 512\n",
    "        self.epochs = epochs\n",
    "        self.tag2id = tag2id\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def preparation(self, X_train, y_train, X_valid, y_valid, label_all_tokens=False):\n",
    "        # Создаем датасеты\n",
    "        self.train_set = NERDataset(X_train, y_train, self.tokenizer, self.tag2id, self.max_len, label_all_tokens)\n",
    "        self.valid_set = NERDataset(X_valid, y_valid, self.tokenizer, self.tag2id, self.max_len, label_all_tokens)\n",
    "\n",
    "        # Создаем загрузчики данных\n",
    "        self.train_loader = DataLoader(self.train_set, batch_size=8, shuffle=True)\n",
    "        self.valid_loader = DataLoader(self.valid_set, batch_size=8, shuffle=False)\n",
    "\n",
    "        # Инициализация оптимизатора и шедулера\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=2e-5, correct_bias=False)\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=len(self.train_loader) * self.epochs\n",
    "        )\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss().to(self.device)\n",
    "        \n",
    "        \n",
    "    def fit(self):\n",
    "    self.model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for data in self.train_loader:\n",
    "        input_ids = data[\"input_ids\"].to(self.device)\n",
    "        attention_mask = data[\"attention_mask\"].to(self.device)\n",
    "        targets = data[\"targets\"].to(self.device)\n",
    "\n",
    "        # Обнуляем градиенты оптимизатора\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Прямой проход модели с передачей меток (targets)\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=targets\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss  # Потери, вычисленные моделью\n",
    "        logits = outputs.logits  # Логиты размерностью [batch_size, seq_len, num_labels]\n",
    "\n",
    "        # Получаем предсказания, выбирая метку с наибольшей вероятностью для каждого токена\n",
    "        preds = torch.argmax(logits, dim=2)  # Размерность [batch_size, seq_len]\n",
    "\n",
    "        # Сглаживаем тензоры для удобства вычислений\n",
    "        preds_flat = preds.view(-1)\n",
    "        targets_flat = targets.view(-1)\n",
    "\n",
    "        # Создаем маску для игнорирования паддингов и специальных токенов с меткой -100\n",
    "        active_indices = targets_flat != -100\n",
    "\n",
    "        # Применяем маску к предсказаниям и целевым меткам\n",
    "        active_preds = preds_flat[active_indices]\n",
    "        active_targets = targets_flat[active_indices]\n",
    "\n",
    "        # Вычисляем количество правильных предсказаний и общее количество предсказаний\n",
    "        correct_predictions += torch.sum(active_preds == active_targets).item()\n",
    "        total_predictions += active_targets.shape[0]\n",
    "\n",
    "        # Добавляем текущую потерю в список потерь\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Обратный проход и обновление весов\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "\n",
    "    # Вычисляем среднюю точность и потерю за эпоху\n",
    "    train_acc = correct_predictions / total_predictions\n",
    "    train_loss = np.mean(losses)\n",
    "    return train_acc, train_loss\n",
    "\n",
    "\n",
    "\n",
    "    from seqeval.metrics import classification_report, f1_score\n",
    "\n",
    "    def eval(self):\n",
    "        self.model.eval()\n",
    "        losses = []\n",
    "        true_labels = []\n",
    "        pred_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in self.valid_loader:\n",
    "                input_ids = data[\"input_ids\"].to(self.device)\n",
    "                attention_mask = data[\"attention_mask\"].to(self.device)\n",
    "                targets = data[\"targets\"].to(self.device)\n",
    "\n",
    "                # Прямой проход модели с передачей меток (targets)\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=targets\n",
    "                )\n",
    "\n",
    "                loss = outputs.loss  # Потери, вычисленные моделью\n",
    "                logits = outputs.logits  # Логиты размерностью [batch_size, seq_len, num_labels]\n",
    "\n",
    "                # Получаем предсказания, выбирая метку с наибольшей вероятностью для каждого токена\n",
    "                preds = torch.argmax(logits, dim=2)  # Размерность [batch_size, seq_len]\n",
    "\n",
    "                # Переводим тензоры на CPU и в numpy для последующей обработки\n",
    "                preds = preds.detach().cpu().numpy()\n",
    "                targets = targets.detach().cpu().numpy()\n",
    "\n",
    "                # Добавляем текущую потерю в список потерь\n",
    "                losses.append(loss.item())\n",
    "\n",
    "                # Конвертируем предсказания и целевые метки в списки токенов\n",
    "                for i in range(len(targets)):\n",
    "                    pred_tags = []\n",
    "                    true_tags = []\n",
    "                    for j in range(len(targets[i])):\n",
    "                        if targets[i][j] != -100:\n",
    "                            pred_tags.append(self.id2tag[preds[i][j]])\n",
    "                            true_tags.append(self.id2tag[targets[i][j]])\n",
    "                    pred_labels.append(pred_tags)\n",
    "                    true_labels.append(true_tags)\n",
    "\n",
    "        avg_loss = np.mean(losses)\n",
    "\n",
    "        # Вычисляем F1-score и выводим отчет классификации\n",
    "        f1 = f1_score(true_labels, pred_labels)\n",
    "        report = classification_report(true_labels, pred_labels)\n",
    "        print(\"Validation Loss: {:.4f}\".format(avg_loss))\n",
    "        print(\"Validation F1-Score: {:.4f}\".format(f1))\n",
    "        print(report)\n",
    "\n",
    "        return f1, avg_loss\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "    best_f1 = 0.0\n",
    "    for epoch in range(self.epochs):\n",
    "        print(f'Epoch {epoch + 1}/{self.epochs}')\n",
    "        \n",
    "        train_f1, train_loss = self.fit()\n",
    "        print(f'Train Loss: {train_loss:.4f} F1-Score: {train_f1:.4f}')\n",
    "\n",
    "        val_f1, val_loss = self.eval()\n",
    "        print(f'Validation Loss: {val_loss:.4f} F1-Score: {val_f1:.4f}')\n",
    "        print('-' * 50)\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            torch.save(self.model.state_dict(), self.model_save_path)\n",
    "            best_f1 = val_f1\n",
    "\n",
    "    # Загрузка наилучшей модели после обучения\n",
    "    self.model.load_state_dict(torch.load(self.model_save_path))\n",
    "    self.model.to(self.device)\n",
    "    \n",
    "    def predict(self, text):\n",
    "    # Шаг 1: Токенизация входного текста\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=self.max_len,\n",
    "        return_token_type_ids=False,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    input_ids = encoding['input_ids'].to(self.device)\n",
    "    attention_mask = encoding['attention_mask'].to(self.device)\n",
    "\n",
    "    # Шаг 2: Получение предсказаний модели\n",
    "    with torch.no_grad():\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "    # Шаг 3: Преобразование логитов в предсказанные метки\n",
    "    logits = outputs.logits  # или outputs[0], в зависимости от модели\n",
    "    predictions = torch.argmax(logits, dim=2)\n",
    "\n",
    "    # Перенос на CPU и преобразование в numpy\n",
    "    predictions = predictions.cpu().numpy()[0]\n",
    "    input_ids = input_ids.cpu().numpy()[0]\n",
    "\n",
    "    # Шаг 4: Преобразование идентификаторов токенов в сами токены\n",
    "    tokens = self.tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    # Шаг 5: Преобразование идентификаторов меток в названия меток\n",
    "    predicted_labels = [self.id2tag[pred] for pred in predictions]\n",
    "\n",
    "    # Шаг 6: Обработка токенов и меток, исключая специальные токены\n",
    "    final_tokens = []\n",
    "    final_labels = []\n",
    "\n",
    "    for token, label in zip(tokens, predicted_labels):\n",
    "        if token not in self.tokenizer.all_special_tokens:\n",
    "            final_tokens.append(token)\n",
    "            final_labels.append(label)\n",
    "\n",
    "    # Шаг 7: Возвращение токенов и соответствующих им предсказанных меток\n",
    "    return list(zip(final_tokens, final_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c917f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предполагается, что вы уже обучили модель и у вас есть объект trainer\n",
    "# trainer = Trainer(...)\n",
    "\n",
    "# Получаем предсказания на валидационном наборе\n",
    "predictions, labels, _ = trainer.predict(valid_dataset)\n",
    "\n",
    "# Преобразуем логиты в предсказанные метки\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Список меток в вашей задаче NER\n",
    "label_list = [...]  # Замените на ваш список меток"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967772e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report, f1_score, accuracy_score\n",
    "\n",
    "# Генерируем отчет по метрикам\n",
    "report = classification_report(true_labels, true_predictions)\n",
    "print(report)\n",
    "\n",
    "# Вычисляем F1-score\n",
    "f1 = f1_score(true_labels, true_predictions)\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "# Вычисляем точность\n",
    "accuracy = accuracy_score(true_labels, true_predictions)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8f6bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Собираем данные для сохранения\n",
    "data = []\n",
    "for i, (tokens, preds, labels) in enumerate(zip(X_valid, true_predictions, true_labels)):\n",
    "    for token, pred, label in zip(tokens, preds, labels):\n",
    "        data.append({\n",
    "            'sentence_id': i,\n",
    "            'token': token,\n",
    "            'true_label': label,\n",
    "            'predicted_label': pred\n",
    "        })\n",
    "\n",
    "# Создаем DataFrame и сохраняем в CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(f\"ner_predictions_fold_{i}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4a8129",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
