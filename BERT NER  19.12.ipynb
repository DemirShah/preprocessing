{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6c98a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Импортируем дополнительные библиотеки\n",
    "import pandas as pd\n",
    "from seqeval.metrics import f1_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Шаг 1: Загрузка и подготовка данных\n",
    "# Предположим, у вас есть данные в формате DataFrame с колонками 'sentence' и 'labels'\n",
    "\n",
    "# Пример загрузки данных из файла CSV\n",
    "# df = pd.read_csv('ner_dataset.csv')\n",
    "\n",
    "# Здесь мы создадим пример данных\n",
    "data = [\n",
    "    {'sentence': 'Джон Смит работает в компании OpenAI в Сан-Франциско.', \n",
    "     'labels': ['B-PER', 'I-PER', 'O', 'O', 'O', 'B-ORG', 'O', 'B-LOC', 'O']},\n",
    "    {'sentence': 'Мария поехала в Москву.', \n",
    "     'labels': ['B-PER', 'O', 'O', 'B-LOC', 'O']},\n",
    "    # Добавьте больше примеров данных\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Шаг 2: Создание списков предложений и меток\n",
    "sentences = df['sentence'].tolist()\n",
    "labels = df['labels'].tolist()\n",
    "\n",
    "# Шаг 3: Загрузка токенизатора и модели\n",
    "model_name = 'bert-base-multilingual-cased'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "# Создаем словарь меток\n",
    "unique_tags = set(tag for doc in labels for tag in doc)\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbb1bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Шаг 4: Подготовка данных для модели\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_len, tag2id):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.tag2id = tag2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        word_labels = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            is_split_into_words=False,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_offsets_mapping=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        labels = np.ones(self.max_len, dtype=int) * -100  # Инициализируем метки как -100\n",
    "\n",
    "        offsets = encoding['offset_mapping'][0].numpy()\n",
    "        input_ids = encoding['input_ids'][0]\n",
    "\n",
    "        label_index = 0\n",
    "        for idx, (start, end) in enumerate(offsets):\n",
    "            if start == end:\n",
    "                continue  # Идентификатор без позиции в исходном тексте (например, специальные токены)\n",
    "            if list(self.tokenizer.decode([input_ids[idx]]))[0] == ' ':\n",
    "                continue  # Пропускаем пробельные символы\n",
    "\n",
    "            if label_index < len(word_labels):\n",
    "                labels[idx] = self.tag2id[word_labels[label_index]]\n",
    "                label_index += 1\n",
    "\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(labels)\n",
    "\n",
    "        return item\n",
    "\n",
    "# Шаг 5: Разбиение данных на обучающую и валидационную выборки\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(\n",
    "    sentences, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Шаг 6: Создание датасетов и загрузчиков данных\n",
    "max_len = 50\n",
    "batch_size = 16\n",
    "\n",
    "train_dataset = NERDataset(train_sentences, train_labels, tokenizer, max_len, tag2id)\n",
    "val_dataset = NERDataset(val_sentences, val_labels, tokenizer, max_len, tag2id)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Шаг 7: Инициализация модели\n",
    "num_labels = len(tag2id)\n",
    "model = BertForTokenClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "# Шаг 8: Определение оптимизатора\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Шаг 9: Определение устройства (CPU или GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9bf3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Шаг 10: Создание класса BERTNERTrainer (если он еще не создан)\n",
    "# Предположим, что вы уже определили класс BERTNERTrainer с методами __init__, fit, eval и train\n",
    "\n",
    "# Вот пример минимальной реализации класса BERTNERTrainer:\n",
    "\n",
    "class BERTNERTrainer:\n",
    "    def __init__(self, model, optimizer, train_loader, valid_loader, tokenizer, tag2id, id2tag, device, epochs=3, model_save_path='best_model.bin', max_len=50):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tag2id = tag2id\n",
    "        self.id2tag = id2tag\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "        self.model_save_path = model_save_path\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def fit(self):\n",
    "        self.model.train()\n",
    "        losses = []\n",
    "        true_labels = []\n",
    "        pred_labels = []\n",
    "\n",
    "        for data in self.train_loader:\n",
    "            input_ids = data['input_ids'].to(self.device)\n",
    "            attention_mask = data['attention_mask'].to(self.device)\n",
    "            labels = data['labels'].to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            preds = torch.argmax(logits, dim=2)\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            labels = labels.detach().cpu().numpy()\n",
    "\n",
    "            for i in range(len(labels)):\n",
    "                pred_tags = []\n",
    "                true_tags = []\n",
    "                for j in range(len(labels[i])):\n",
    "                    if labels[i][j] != -100:\n",
    "                        pred_tags.append(self.id2tag[preds[i][j]])\n",
    "                        true_tags.append(self.id2tag[labels[i][j]])\n",
    "                pred_labels.append(pred_tags)\n",
    "                true_labels.append(true_tags)\n",
    "\n",
    "        avg_loss = np.mean(losses)\n",
    "        f1 = f1_score(true_labels, pred_labels)\n",
    "        return f1, avg_loss\n",
    "\n",
    "    def eval(self):\n",
    "        self.model.eval()\n",
    "        losses = []\n",
    "        true_labels = []\n",
    "        pred_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in self.valid_loader:\n",
    "                input_ids = data[\"input_ids\"].to(self.device)\n",
    "                attention_mask = data[\"attention_mask\"].to(self.device)\n",
    "                labels = data[\"labels\"].to(self.device)\n",
    "\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "\n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "\n",
    "                losses.append(loss.item())\n",
    "\n",
    "                preds = torch.argmax(logits, dim=2)\n",
    "                preds = preds.detach().cpu().numpy()\n",
    "                labels = labels.detach().cpu().numpy()\n",
    "\n",
    "                for i in range(len(labels)):\n",
    "                    pred_tags = []\n",
    "                    true_tags = []\n",
    "                    for j in range(len(labels[i])):\n",
    "                        if labels[i][j] != -100:\n",
    "                            pred_tags.append(self.id2tag[preds[i][j]])\n",
    "                            true_tags.append(self.id2tag[labels[i][j]])\n",
    "                    pred_labels.append(pred_tags)\n",
    "                    true_labels.append(true_tags)\n",
    "\n",
    "        avg_loss = np.mean(losses)\n",
    "        f1 = f1_score(true_labels, pred_labels)\n",
    "        report = classification_report(true_labels, pred_labels)\n",
    "        print(\"Validation Loss: {:.4f}\".format(avg_loss))\n",
    "        print(\"Validation F1-Score: {:.4f}\".format(f1))\n",
    "        print(report)\n",
    "        return f1, avg_loss\n",
    "\n",
    "    def train(self):\n",
    "        best_f1 = 0.0\n",
    "        for epoch in range(self.epochs):\n",
    "                        print(f'Epoch {epoch + 1}/{self.epochs}')\n",
    "            \n",
    "            train_f1, train_loss = self.fit()\n",
    "            print(f'Train Loss: {train_loss:.4f} F1-Score: {train_f1:.4f}')\n",
    "\n",
    "            val_f1, val_loss = self.eval()\n",
    "            print(f'Validation Loss: {val_loss:.4f} F1-Score: {val_f1:.4f}')\n",
    "            print('-' * 50)\n",
    "\n",
    "            if val_f1 > best_f1:\n",
    "                torch.save(self.model.state_dict(), self.model_save_path)\n",
    "                best_f1 = val_f1\n",
    "\n",
    "        # Загрузка наилучшей модели после обучения\n",
    "        self.model.load_state_dict(torch.load(self.model_save_path))\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def predict(self, text):\n",
    "        # Шаг 1: Токенизация входного текста\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].to(self.device)\n",
    "        attention_mask = encoding['attention_mask'].to(self.device)\n",
    "\n",
    "        # Шаг 2: Получение предсказаний модели\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "        # Шаг 3: Преобразование логитов в предсказанные метки\n",
    "        logits = outputs.logits  # или outputs[0], в зависимости от модели\n",
    "        predictions = torch.argmax(logits, dim=2)\n",
    "\n",
    "        # Перенос на CPU и преобразование в numpy\n",
    "        predictions = predictions.cpu().numpy()[0]\n",
    "        input_ids = input_ids.cpu().numpy()[0]\n",
    "\n",
    "        # Шаг 4: Преобразование идентификаторов токенов в сами токены\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "        # Шаг 5: Преобразование идентификаторов меток в названия меток\n",
    "        predicted_labels = [self.id2tag[pred] for pred in predictions]\n",
    "\n",
    "        # Шаг 6: Обработка токенов и меток, исключая специальные токены\n",
    "        final_tokens = []\n",
    "        final_labels = []\n",
    "\n",
    "        for token, label in zip(tokens, predicted_labels):\n",
    "            if token not in self.tokenizer.all_special_tokens:\n",
    "                final_tokens.append(token)\n",
    "                final_labels.append(label)\n",
    "\n",
    "        # Шаг 7: Возвращение токенов и соответствующих им предсказанных меток\n",
    "        return list(zip(final_tokens, final_labels))\n",
    "\n",
    "# Шаг 11: Создание объекта trainer\n",
    "trainer = BERTNERTrainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=val_loader,\n",
    "    tokenizer=tokenizer,\n",
    "    tag2id=tag2id,\n",
    "    id2tag=id2tag,\n",
    "    device=device,\n",
    "    epochs=3,  # Вы можете изменить количество эпох\n",
    "    model_save_path='best_ner_model.bin',\n",
    "    max_len=max_len\n",
    ")\n",
    "\n",
    "# Шаг 12: Обучение модели\n",
    "trainer.train()\n",
    "\n",
    "# После обучения вы можете использовать метод predict для предсказания\n",
    "test_sentence = \"Иван Иванович посетил Нью-Йорк в прошлом году.\"\n",
    "predictions = trainer.predict(test_sentence)\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
